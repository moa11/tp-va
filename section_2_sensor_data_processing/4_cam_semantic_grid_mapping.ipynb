{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Header Image](../assets/header_image.png \"Header Image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bienvenue dans l'exercice **Cartographie Sémantique par Grille basée sur Caméra**.\n",
    "\n",
    "Dans cet exercice, nous allons utiliser des images prises par plusieurs caméras montées sur le véhicule pour obtenir une **vue aérienne (BEV) à 360°** de la route. Notre approche utilise la **transformation de perspective inverse (IPM)**.\n",
    "\n",
    "Ensuite, nous appliquerons l'IPM à des images segmentées sémantiquement pour obtenir une **carte de grille sémantique** de l'environnement du véhicule, comme illustré dans l'image ci-dessous.\n",
    "\n",
    "Les cartes de grille sémantiques sont similaires aux cartes de grille d'occupation et fournissent des informations supplémentaires sur le type d'objets présents dans l'environnement d'un véhicule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Example](ipm_assets/images/demo_carla.png \"Example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La carte de grille sémantique que nous allons obtenir peut faciliter les tâches de planification et de prédiction.\n",
    "\n",
    "Dans cet exercice, nous allons parcourir les étapes suivantes :\n",
    "\n",
    "- Charger les images et les paramètres de la caméra\n",
    "- Utiliser OpenCV pour appliquer la transformation de perspective inverse\n",
    "- Utiliser le modèle de caméra sténopé\n",
    "- Appliquer les transformations de système de coordonnées\n",
    "- Effectuer l'IPM\n",
    "- Assembler plusieurs images en vue aérienne BEV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commençons par importer tous les packages nécessaires pour cet exercice :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(img, title=None):\n",
    "    \"\"\"this function shows an opencv image in this notebook\"\"\"\n",
    "    rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(rgb_img)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemple des Entrées\n",
    "\n",
    "Nous allons commencer cet exercice en appliquant **l'IPM à des images RGB** (non segmentées) capturées par des caméras montées sur le véhicule dans un simulateur (VTD). L'image chargée par le segment de code suivant a été capturée par la caméra avant droite du véhicule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read image\n",
    "img = cv2.imread(\"ipm_assets/images/vr_1.png\")\n",
    "# show image \n",
    "show_image(img, \"Original Image\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Tâche : Utiliser OpenCV pour effectuer l'IPM\n",
    "Dans cette tâche, vous allez apprendre à utiliser OpenCV pour transformer une image, prise par une caméra, en une image de vue aérienne, comme illustré dans l'image ci-dessous.\n",
    "\n",
    "![Expected Output](ipm_assets/images/expected_output_warp_persp.png \"Expected Output\")\n",
    "\n",
    "Cette transformation peut être réalisée en utilisant une transformation homogène **P** (que nous appellerons `P_cam_to_bev` dans notre code).\n",
    "Cette transformation peut mapper un point (en coordonnées d'image de caméra) vers des coordonnées routières (coordonnées BEV).\n",
    "\n",
    "En utilisant les coordonnées **homogènes** **x<sub>c<sub>i</sub></sub>** pour l'image de caméra et **x<sub>r<sub>i</sub></sub>** pour la route, nous pouvons mapper les points **x<sub>c<sub>i</sub></sub>** vers les points **x<sub>r<sub>i</sub></sub>** en utilisant une multiplication matricielle.\n",
    "\n",
    "La transformation homogène **P** est une **transformation projective** et peut être écrite comme une **matrice 3x3** :\n",
    "\n",
    "\n",
    "$$ \\large P =  \\begin{bmatrix} p_{11} & p_{12} & p_{13} \\\\ p_{21} & p_{22} & p_{23}\\\\p_{31} & p_{32} & 1 \\end{bmatrix}$$\n",
    "\n",
    "Les points en coordonnées d'image sont mappés vers les coordonnées routières comme suit :\n",
    "\n",
    "$$ \\large \\begin{bmatrix} w_i x_{r_i} \\\\ w_i y_{r_i} \\\\ w_i \\end{bmatrix} = P \\begin{bmatrix} x_{c_i} \\\\ y_{c_i} \\\\ 1 \\end{bmatrix}$$\n",
    "\n",
    "\n",
    "En utilisant l'équation ci-dessus et en connaissant **quatre points sources dans les coordonnées d'image** et leurs **quatre points correspondants dans les coordonnées routières**, nous pouvons résoudre un système d'équations pour les 8 paramètres inconnus de **P**.\n",
    "\n",
    "Heureusement, OpenCV fournit une méthode pour faire cela !\n",
    "\n",
    "En utilisant `cv2.getPerspectiveTransform()`, qui prend `source_coordinates` (**x<sub>c<sub>1</sub></sub>, x<sub>c<sub>2</sub></sub>, x<sub>c<sub>3</sub></sub>, x<sub>c<sub>4</sub></sub>**) et `destination_coordinates` (**x<sub>r<sub>1</sub></sub>, x<sub>r<sub>2</sub></sub>, x<sub>r<sub>3</sub></sub>, x<sub>r<sub>4</sub></sub>**) comme paramètres, nous pouvons calculer la matrice d'une transformation à partir des points sources fournis vers les points de destination. En interne, cette fonction applique l'élimination gaussienne (par défaut, d'autres méthodes se trouvent [ici](https://docs.opencv.org/master/d2/de8/group__core__array.html#gaaf9ea5dcc392d5ae04eacb9920b9674c)) pour calculer les éléments de la matrice de transformation de perspective.\n",
    "\n",
    "Votre tâche est de fournir 4 points sources et 4 points de destination pour `getPerspectiveTransform()`.\n",
    "Les points sources doivent être fournis en coordonnées d'image (x<sub>c<sub>i</sub></sub>,y<sub>c<sub>i</sub></sub>).\n",
    "\n",
    "Une façon pratique d'y parvenir est de choisir les points de manière à ce qu'ils forment un rectangle en vue de dessus.\n",
    "\n",
    "Ensuite, vous appliquerez la matrice de transformation résultante pour produire une image BEV en utilisant `cv2.warpPerpective()`.\n",
    "\n",
    "Remplacez les espaces réservés `None` par votre code.\n",
    "\n",
    "#### __Conseils__ :\n",
    "- Les points sources et de destination doivent être fournis sous forme de listes Python\n",
    "- (Uniquement pour **l'utilisation locale**, ne fonctionne pas dans JupyterHub) `cv2.show()` peut être utilisé pour afficher l'image d'entrée. Survolez l'image. La position actuelle de la souris en coordonnées d'image est affichée dans le coin inférieur gauche. Utilisez ceci pour obtenir les coordonnées des points sources. Vous pouvez augmenter le temps dans `cv2.waitKey()` (spécifié en millisecondes), qui définit la durée d'affichage du graphique. Utilisez `cv2.getPerspectiveTransform()` pour calculer la matrice de transformation de perspective inverse. Consultez la [documentation](https://docs.opencv.org/4.x/da/d54/group__imgproc__transform.html#ga20f62aa3235d869c9956436c870893ae) pour plus de détails.\n",
    "- Utilisez `cv2.warpPerpective()` pour appliquer la matrice de transformation de perspective calculée à l'image d'entrée. Consultez la [documentation](https://docs.opencv.org/4.x/da/d54/group__imgproc__transform.html#gaf73673a7e8e18ec6963e3774e6a94b87) pour plus de détails.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of output image\n",
    "height, width = 600, 400\n",
    "\n",
    "### START CODE HERE ###\n",
    "\n",
    "# plot the input image\n",
    "#cv2.imshow(\"Input image\", img)\n",
    "#cv2.waitKey(60)\n",
    "#cv2.destroyAllWindows()\n",
    "\n",
    "# define the source points\n",
    "x_c_1, x_c_2, x_c_3, x_c_4 = None, None, None, None\n",
    "\n",
    "src_pts = np.float32([x_c_1, x_c_2, x_c_3, x_c_4])\n",
    "# define the destination points \n",
    "x_r_1, x_r_2, x_r_3, x_r_4 = None, None, None, None\n",
    "\n",
    "dst_pts =np.float32([x_r_1, x_r_2, x_r_3, x_r_4])\n",
    "\n",
    "# calculate the perspective transform matrix\n",
    "P_cam_to_bev = None\n",
    "#print (P_cam_to_bev)\n",
    "\n",
    "# caculate the output image\n",
    "output = None\n",
    "\n",
    "### END CODE HERE ###\n",
    "show_image(output, \"Output Image\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we will draw a trapezoid connecting the source points in the image and apply the calculated transformation to the image again to see how the trapezoid is mapped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to draw a line\n",
    "def draw_line(image, pt1, pt2, color=(0,0,255), thickness=5):\n",
    "    print(pt1, pt2)\n",
    "    cv2.line(image, pt1.astype(int), pt2.astype(int), color, thickness)\n",
    "    \n",
    "# function to draw rectanges:\n",
    "def draw_rectange(image, pts, color=(0,0,255), thickness=5):\n",
    "    draw_line(image, pts[0], pts[1], color, thickness)\n",
    "    draw_line(image, pts[1], pts[2], color, thickness)\n",
    "    draw_line(image, pts[2], pts[3], color, thickness)\n",
    "    draw_line(image, pts[3], pts[0], color, thickness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw a rectange that shows the source points\n",
    "draw_rectange(img, src_pts)\n",
    "# draw a rectangle that shows the destination points\n",
    "#draw_rectange(output, dst_pts)\n",
    "# caculate the output image\n",
    "output =cv2.warpPerspective(img ,P_cam_to_bev, (width,height))\n",
    "\n",
    "\n",
    "show_image(img, \"Input Image\")\n",
    "show_image(output, \"Output Image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette méthode ne fonctionne pas uniquement pour la transformation d'images en vue aérienne. Elle peut transformer n'importe quelle vue en une autre, tant que les points sources et de destination sont coplanaires."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IPM utilisant la Géométrie Projective\n",
    "Comme la méthode utilisant OpenCV n'est pas exacte (car les points sources et de destination doivent être définis manuellement), une méthode basée sur la géométrie pour calculer l'image de vue aérienne en utilisant des transformations de système de coordonnées sera discutée.\n",
    "\n",
    "La méthode basée sur la géométrie sera fondée sur un modèle de caméra simple et utilisera les matrices intrinsèques et extrinsèques pour calculer la matrice de transformation de perspective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modèle de Caméra\n",
    "Nous allons commencer par introduire le modèle de caméra simple que nous utiliserons dans cet exercice.\n",
    "Ce modèle définit comment un point $\\\\textbf{X}=(X,Y,Z)$ dans le repère monde est transformé en un point $\\\\textbf{x}_{cam}=(x_c,y_c)$ dans l'image de la caméra.\n",
    "\n",
    "Ce modèle utilise les paramètres intrinsèques et extrinsèques de la caméra.\n",
    "\n",
    "#### Transformation du repère monde vers le repère caméra\n",
    "Ici, nous **supposons** que le **repère monde** est le **repère principal du véhicule** puisque notre objectif est de produire une image de vue aérienne ou une carte de grille sémantique **centrée sur le véhicule**. Le repère de coordonnées principal du véhicule est souvent appelé le __base link__. Il peut être utilisé pour relier les coordonnées du véhicule aux coordonnées de la carte.\n",
    "\n",
    "Nous définissons une transformation homogène **E** qui transforme les points du repère monde vers le repère caméra (toujours en trois dimensions) comme montré dans l'équation suivante :\n",
    "\n",
    "$$ \\large X_{cam} = E X $$\n",
    "\n",
    "\n",
    "où **X** représente la coordonnée homogène du point (en trois dimensions) dans le repère monde (ici aussi le repère principal du véhicule) et **X<sub>cam</sub>** représente la coordonnée homogène du point (en trois dimensions) dans le repère caméra.\n",
    "\n",
    "Cette matrice de transformation homogène incorpore les **paramètres extrinsèques** de la caméra.\n",
    "Nous appellerons cette matrice la **matrice extrinsèque**.\n",
    "Elle dépend de la pose de la caméra par rapport à un repère de référence. Dans notre cas, la matrice extrinsèque transforme le repère de coordonnées principal du véhicule vers le repère de coordonnées de la caméra.\n",
    "\n",
    "**E** peut être composée d'une matrice de rotation 3x3 **R** et d'un vecteur de translation 3x1 **t**.\n",
    "     \n",
    "\n",
    "$$ \\large E = \\begin{bmatrix} R & t \\\\ 0^T & 1\\end{bmatrix} = \\begin{bmatrix} R & -R \\tilde{C} \\\\ 0^T & 1\\end{bmatrix}$$\n",
    "\n",
    "Où $\\tilde{C}$ est la coordonnée du repère dans le repère principal du véhicule.\n",
    "\n",
    "Maintenant que nous pouvons mapper les points vers le repère caméra, nous allons les projeter dans le plan image.\n",
    "\n",
    "\n",
    "#### Transformation du repère caméra vers le plan image\n",
    "Nous définissons une transformation projective **K** qui transforme les points tridimensionnels dans les coordonnées du repère caméra en points bidimensionnels dans les coordonnées de l'image. Cette transformation peut également être définie comme une matrice, qui dépend des paramètres de la caméra, comme la distance focale $f$ et le point principal $p$. (Ici, nous ignorons la distorsion de l'objectif).\n",
    "Nous appellerons cette matrice la **matrice intrinsèque**.\n",
    "Les coordonnées homogènes d'un point tridimensionnel dans le repère caméra sont mappées vers les coordonnées homogènes d'un point bidimensionnel dans le plan image selon l'équation suivante :\n",
    "\n",
    "$$ \\large x_{cam} = K [I|0] X_{cam} = K \\begin{bmatrix} 1 & 0 & 0 & 0\\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0\\end{bmatrix} X_{cam} $$\n",
    "\n",
    "où **X<sub>cam</sub>** sont les coordonnées homogènes du point (tridimensionnel) dans le repère caméra et **x<sub>cam</sub>** est la coordonnée homogène du point (bidimensionnel) dans le plan caméra.\n",
    "\n",
    "\n",
    "**K** peut être écrite comme suit :\n",
    "\n",
    "$$ \\large K = \\begin{bmatrix} f_x & 0 & p_x \\\\ 0 & f_y & p_y \\\\ 0 & 0 & 1\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "### Matrice de projection de la caméra\n",
    "En utilisant les matrices de transformation intrinsèques et extrinsèques, nous pouvons définir la transformation qui mappe les points du système de coordonnées du véhicule vers le système de coordonnées du plan image, selon l'équation suivante :\n",
    "\n",
    "\n",
    "$$ \\large x_{cam} = P X$$\n",
    "\n",
    "La matrice de projection **P** peut être calculée en multipliant **K** et **E** :\n",
    "\n",
    "$$ \\large P = K \\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0\\end{bmatrix} E $$\n",
    "\n",
    "La matrice $\\large [I | 0]$ est nécessaire car nous n'avons besoin que des coordonnées __inhomogènes__ du point __X__ dans le repère monde."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tâche : Obtenir la matrice intrinsèque de la caméra\n",
    "Les paramètres de la caméra peuvent être estimés en effectuant un étalonnage de la caméra.\n",
    "Ici, l'étalonnage de la caméra a déjà été effectué et les paramètres sont fournis dans un fichier externe.\n",
    "L'objectif de cette tâche est d'extraire ces paramètres du fichier externe.\n",
    "\n",
    "Votre tâche ici est de lire la matrice intrinsèque à partir d'un fichier json.\n",
    "Pour cela, vous allez implémenter une fonction qui obtient un chemin vers un fichier JSON et retourne un dictionnaire contenant les paramètres intrinsèques de toutes les caméras dans le fichier sous forme de **tableaux NumPy**.\n",
    "Complétez la fonction `get_intrinsics()` en remplaçant les espaces réservés `None`.\n",
    "#### __Conseils :__\n",
    "- Inspectez le fichier `intrinsics.json` fourni sous `ipm_assets/cameras/intrinsics.json`.\n",
    "- Utilisez `open()` de Python pour lire le fichier json. Consultez la [documentation](https://docs.python.org/3/library/functions.html#open).\n",
    "- Utilisez la fonction `json.load()`. Consultez la [documentation](https://docs.python.org/3/library/json.html).\n",
    "- après la lecture du fichier, vous devriez obtenir un dictionnaire qui contient la matrice intrinsèque de **8 caméras** montées sur un véhicule. Dans les fichiers json, les caméras sont nommées : **('vr_1', 'vl_1', 'hr_1', 'hl_1', 'vr_2', 'vl_2', 'hr_2', 'hl_2')**\n",
    "- Notez que les matrices chargées sont sauvegardées sous forme de listes Python et doivent être converties en tableaux NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intrinsics(f_path=\"ipm_assets/cameras/intrinsics.json\"):\n",
    "    ### START CODE HERE ###\n",
    "    file = None\n",
    "    intrinsics_dict = None\n",
    "    for camera_n, intrinsic_matrix in intrinsics_dict.items():\n",
    "        intrinsics_dict[None] = np.array(None)\n",
    "    ### END CODE HERE ###\n",
    "    return intrinsics_dict\n",
    "\n",
    "intrinsics_dict = get_intrinsics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary we loaded contains the intrinsic matrices of 8 different cameras. Let's print the intrinsic matrix of the **front right camera**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the intrinsics of the first front facing camera (vr_1)\n",
    "K = np.array(intrinsics_dict['vr_1'])\n",
    "K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Calculate the camera extrinsic matrix\n",
    "The goal of this task is to compute the extrinsic matrix of the camera.\n",
    "\n",
    "Your task here is to read the extrinsic camera parameters from a JSON file.\n",
    "The provided JSON files contain extrinsic information about multiple camera-mounted vehicles. \n",
    "This information contains the translation of each camera frame w.r.t the vehicle frame.\n",
    "It also contains the orientation of each camera frame w.r.t the vehicle frame expressed in roll, pitch yaw angles.\n",
    "\n",
    "The **position** and **orientation** of the camera need to be converted into a homogeneous transformation. \n",
    "\n",
    "\n",
    "The goal here is to implement a function that gets a path to a JSON file and returns a dictionary containing the homogeneous transformations describing the extrinsic parameters of all cameras in the file as NumPy arrays.\n",
    "\n",
    "Replace the `None` placeholders with your code.\n",
    "#### __Hints:__\n",
    "- Read the translation vector (saved in the dictionary as 'translation') and convert it to a NumPy array.\n",
    "- Read (roll, pitch, yaw) (saved in the dictionary as 'rotation_rpy').\n",
    "- The representation (`roll`, `pitch`, `yaw`) is equivalent to (3x3)-rotation matrices performing the following operations in order:\n",
    "    - a rotation around the x-axis with the roll angle\n",
    "    - a rotation around the y-axis with the pitch angle\n",
    "    - a rotation around the z-axis with the yaw angle\n",
    "- Combine into a homogeneous transform (4x4)\n",
    "    - use [`numpy.column_stack()`](https://numpy.org/doc/stable/reference/generated/numpy.column_stack.html) and [`numpy.row_stack()`](https://numpy.org/devdocs/reference/generated/numpy.row_stack.html)\n",
    "    - Start by performing a column-wise stacking of the rotation matrix and the translation vector. The result should be a 3x4 matrix,\n",
    "    - then perform a row-wise stacking of the resulting 3x4 matrix and the array `np.array([0., 0., 0., 1.])`. The result should be a 4x4 matrix. \n",
    "    - The result should be a dictionary that contains camera names as keys and their corresponding extrinsic matrices as values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "def get_extrinsics(f_path=\"ipm_assets/cameras/extrinsics_rpy.json\"):\n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # read json file\n",
    "    file = None\n",
    "    extrinsics_rpy_dict = None\n",
    "\n",
    "    # extrinsic dict (here we will save the extrinsic matrix for each camera)\n",
    "    extrinsics_dict = dict()\n",
    "    \n",
    "    for camera_n, extrinsic_params in extrinsics_rpy_dict.items():\n",
    "        # extract translation parameter\n",
    "        t = None\n",
    "        \n",
    "        # get roll, pitch and yaw for the camera 'cmera_n'\n",
    "        roll, pitch, yaw = None\n",
    "        # compute rotation matrix\n",
    "        Rz = None\n",
    "        Ry = None\n",
    "        Rx = None\n",
    "\n",
    "        R = Rz.dot(Ry.dot(Rx))\n",
    "\n",
    "        \n",
    "        # combine translation (3x1) and roatation matrix (3x3) into a 4x4 homogeneous transform\n",
    "        transform = None\n",
    "        # add 1 row ([0., 0., 0., 1.]) to complete the transform \n",
    "        transform = None\n",
    "        extrinsics_dict[camera_n] = None\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "    return extrinsics_dict\n",
    "\n",
    "extrinsics_dict = get_extrinsics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the extrinsic matrix of the **front right camera**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the extrinsics of the first front facing camera (vr_1)\n",
    "E = extrinsics_dict['vr_1']\n",
    "E\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We can now build the complete camera projection model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Calculate the projection matrix\n",
    "\n",
    "Calculate the camera projection matrix of the front right camera by replacing the `None` placeholder with your code.\n",
    "#### __Hints__:\n",
    "\n",
    "- The output should be a 3x4 matrix\n",
    "- The extrinsic matrix is a 4x4 matrix where the intrinsic matrix is a 3x3 matrix, so don't forget the $[I|0]$ matrix\n",
    "- Remember that we already saved the intrinsic and extrinsic matrices of the front right camera (in `K` and `E`) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "P = None\n",
    "### END CODE HERE ###\n",
    "P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Mapping from  the road (BEV image) coordinates to the vehicle's coordinates\n",
    "\n",
    "Now we will define the mapping **M** from (two-dimensional) BEV image plane points (in homogeneous road coordinates) to (three-dimensional) world points (in homogeneous vehicle frame coordinates). \n",
    "In its simplest form, the mapping would have the following equation:\n",
    "\n",
    "$$\\large  M_{\\text{2Dto3D}} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 1\\end{bmatrix} $$\n",
    "\n",
    "\n",
    "The equaion for $M_{\\text{2Dto3D}}$ is based on the assumption that *z<sub>r</sub> = 0* (all points on the road plane have the height zero).\n",
    "\n",
    "In addition to *z<sub>r</sub> = 0* , this equation also requires that the coordinate system of the road and the vehicle are colocated and have the same orientation. \n",
    "Since not all of these conditions are satisfied, we have to adjust M.\n",
    "The figure below shows that the two frames are not colocated and don't have the same orientation.\n",
    "\n",
    "![Expected Output](ipm_assets/images/road_to_vehicle.png \"OpenCV cooridnates\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paramètres\n",
    "\n",
    "Définissons d'abord quelques paramètres pour le mapping **M**.\n",
    "- `px_per_m` est la résolution et définit le nombre de pixels utilisés pour dessiner 1m. Cette résolution est utilisée verticalement et horizontalement. Dans notre cas, 1px correspond à 0.1m².\n",
    "- `output_width` et `output_height` sont les dimensions de l'image BEV.\n",
    "- `shift_x` et `shift_y` spécifient de combien l'origine du repère véhicule est décalée par rapport au repère route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for ipm\n",
    "# output resolution\n",
    "px_per_m = 10 # number of pixels per meter\n",
    "# output size\n",
    "output_width = 798\n",
    "output_height = 400\n",
    "# shift to center of output image\n",
    "shift_x = output_width/ 2.0 \n",
    "shift_y = output_height / 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajouter l'opération de décalage :\n",
    "\n",
    "Le premier ajustement que vous devrez apporter au mapping **M**, du repère de coordonnées de la route vers le repère de coordonnées du véhicule, est le **décalage** du repère de coordonnées de la route vers le repère de coordonnées du véhicule.\n",
    "\n",
    "Pour cela, **remplacez** l'espace réservé `None` par votre code pour définir une matrice de transformation homogène qui effectue une translation de (x<sub>shift</sub>, y<sub>shift</sub>) par rapport au repère route.\n",
    "\n",
    "##### __Conseils :__\n",
    "- La matrice homogène incorporant une translation de (x<sub>shift</sub>, y<sub>shift</sub>) peut être écrite comme\n",
    "\n",
    "$$ \\large M_{\\text{shift}} = \\begin{bmatrix} 1 & 0 & x_{\\text{shift}}\\\\ \n",
    "0 & 1 & y_{\\text{shift}} \\\\ \n",
    "0 & 0 & 1 \\end{bmatrix} $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "M_shift = None\n",
    "### END CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changer la direction de l'axe y :\n",
    "\n",
    "Le deuxième ajustement que vous devrez apporter au mapping **M** est le **miroir** du repère de coordonnées de la route sur l'axe x de sorte que l'axe y des deux repères correspondent.\n",
    "\n",
    "Pour cela, **remplacez** l'espace réservé `None` par votre code pour définir une matrice de transformation homogène qui effectue le miroir.\n",
    "\n",
    "##### __Conseils :__\n",
    "- Avec quelle matrice homogène un point homogène **x** = (x,y,1) doit-il changer le signe de y\n",
    "\n",
    "$$  \\large \\begin{bmatrix} x\\\\ \n",
    "-y\\\\ \n",
    "1\\end{bmatrix} = M_{\\text{direction}}\\begin{bmatrix} x\\\\ \n",
    "y\\\\ \n",
    "1\\end{bmatrix} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "\n",
    "M_direction = None\n",
    "\n",
    "### END CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changer l'échelle de l'axe y :\n",
    "\n",
    "Le deuxième ajustement que vous devrez apporter au mapping **M** serait l'**échelle** des coordonnées dans le repère route. Cette mise à l'échelle dépend de la résolution spécifiée pour la transformation.\n",
    "\n",
    "Pour cela, **remplacez** l'espace réservé `None` par votre code pour définir une matrice de transformation homogène qui effectue la mise à l'échelle.\n",
    "\n",
    "##### __Conseils :__\n",
    "- Une transformation homogène qui effectue une mise à l'échelle des entrées avec la valeur **a** peut être écrite comme :\n",
    "$$ \\large M_{\\text{scale}} = \\begin{bmatrix} a & 0 & 0\\\\ \n",
    "0 & a & 0 \\\\ \n",
    "0 & 0 & 1 \\end{bmatrix} $$\n",
    "\n",
    "- Rappelez-vous que les entrées (coordonnées dans l'image BEV) ont l'unité pixel et (coordonnées dans la route) l'unité mètre\n",
    "- Utilisez `px_per_m` pour la mise à l'échelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "\n",
    "M_scale = None\n",
    "\n",
    "### END CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation M ajustée :\n",
    "La nouvelle transformation M combine `M_2Dto3D` et les opérations de **décalage**, **miroir** et **mise à l'échelle** nécessaires pour l'ajustement.\n",
    "\n",
    "**remplacez** l'espace réservé `None` par votre code pour définir la matrice de transformation ajustée **M**.\n",
    "\n",
    "#### __Conseils__\n",
    "- (Un) ordre valide des opérations : décalage (`M_shift`), miroir(`M_direction`), mise à l'échelle(`M_scale`), puis mapping (`M_2Dto3D`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "# direct mapping (assuming z=0)\n",
    "\n",
    "M_2Dto3D = None\n",
    "# adjusted\n",
    "M = None\n",
    "### END CODE HERE ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the needed parts to build a mapping between camera images and the BEV image, we can compute the **inverse perspective mapping matrix**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que nous avons toutes les parties nécessaires pour construire un mapping entre les images de caméra et l'image BEV, nous pouvons calculer la **matrice de transformation de perspective inverse**.\n",
    "\n",
    "## Tâche : Calculer la matrice de transformation de perspective inverse\n",
    "Calculez le mapping de l'image de la route vers l'image de la caméra avant, puis calculez la matrice inverse, qui transforme les images de la caméra vers la vue de dessus (ou vue route).\n",
    "\n",
    "Remplacez l'espace réservé `None` par votre code.\n",
    "#### __Conseils :__\n",
    "- le mapping de l'image de la route vers l'image de la caméra avant est défini comme suit (nous avons besoin de l'**inverse**)\n",
    "\n",
    "$$ \\\\large x_{cam} = P X = K [I|0] E X= K[R|t] M x_r$$\n",
    "\n",
    "- __X__ est la coordonnée d'un point dans le repère véhicule\n",
    "- __x<sub>r</sub>__ est la coordonnée du même point dans le repère route\n",
    "- __x<sub>cam</sub>__ est la coordonnée du même point en coordonnées d'image\n",
    "- utilisez `numpy.linalg.inv()` (documentation [ici](https://numpy.org/doc/stable/reference/generated/numpy.linalg.inv.html)) pour inverser la matrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "\n",
    "M_ipm = None\n",
    "### END CODE HERE ###\n",
    "\n",
    "print(M_ipm) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tâche : Appliquer la matrice de transformation de perspective inverse sur une image\n",
    "Ici, vous allez utiliser la matrice M_ipm pour appliquer la transformation de perspective.\n",
    "Vous pouvez utiliser cv2.warpPerspective() qui prend en entrée une image et une matrice de mapping (3x3).\n",
    "Cette fonction applique la matrice d'homographie à chaque pixel de l'image d'entrée.\n",
    "Pour chaque position (x<sub>c</sub>,y<sub>c</sub>) dans l'image d'entrée, (x<sub>r</sub>,y<sub>r</sub>) est calculé.\n",
    "cv2.warpPerspective() applique également l'interpolation.\n",
    "\n",
    "Remplacez l'espace réservé `None` par votre code.\n",
    "\n",
    "#### __Conseils :__\n",
    "- lisez l'image depuis **\"ipm_assets/images/vr_1.png\"**. Utilisez [`cv2.imread()`](https://docs.opencv.org/3.4/d4/da8/group__imgcodecs.html)\n",
    "- utilisez la fonction de transformation de perspective d'OpenCV : `cv2.warpPerspective()`\n",
    "- sortie attendue\n",
    "\n",
    "![Expected Output](ipm_assets/images/ipm_invalid.png \"ipm output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "# read image\n",
    "image = None\n",
    "\n",
    "# use M_ipm to perform the perspective mapping\n",
    "img_out = None\n",
    "### END CODE HERE ###\n",
    "\n",
    "show_image(img_out, \"Output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le mapping que nous avons conçu ne tient pas pour les pixels au-dessus du niveau du sol.\n",
    "C'est pourquoi, par exemple, les bâtiments sont mappés incorrectement.\n",
    "\n",
    "Vous pouvez atténuer cet effet en masquant simplement la partie supérieure de l'image d'entrée (qui ne montre pas la route).\n",
    "\n",
    "Remplacez les espaces réservés `None` par votre code.\n",
    "\n",
    "\n",
    "#### __Conseils__ :\n",
    "- Définissez une matrice de zéros (tableau numpy) **mask**, qui a la hauteur et la largeur de l'image d'entrée. Utilisez **dtype=\"uint8\"**\n",
    "- Dessinez un rectangle sur le masque qui a la couleur blanc=255 et couvre la partie de l'image que nous ne voulons pas masquer (moitié inférieure)\n",
    "- Utilisez **cv2.bitwise_and(image, image, mask=mask)** pour appliquer le masque à l'entrée\n",
    "- Sortie attendue :\n",
    "\n",
    "![Expected Output](ipm_assets/images/cropped_image.png \"Cropped image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "\n",
    "# extract the height and width of the input image \n",
    "input_img_height, input_img_width, _ = None\n",
    "# intialize the mask with np.zeros  \n",
    "mask = None\n",
    "# rectangle for the region of intrest\n",
    "cv2.rectangle(mask, None, None, None,  255, -1)\n",
    "# apply mask\n",
    "image = cv2.bitwise_and(image, image, mask=None)\n",
    "\n",
    "### END CODE HERE ###\n",
    "\n",
    "# show the image\n",
    "show_image(image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tâche : Tout mettre ensemble\n",
    "\n",
    "Pour éviter de refaire tout le processus de calcul de la matrice IPM manuellement pour différentes caméras, nous pouvons mettre tout ce que nous avons fait dans une fonction qui prend comme entrées :\n",
    "\n",
    "- image\n",
    "- matrice extrinsèque de la caméra\n",
    "- matrice intrinsèque de la caméra\n",
    "- un dictionnaire de configuration (hauteur de sortie, largeur, résolution, etc.)\n",
    "\n",
    "et sorties :\n",
    "- image BEV\n",
    "\n",
    "Remplacez les espaces réservés `None` par votre code.\n",
    "\n",
    "##### __Conseils :__\n",
    "- n'oubliez pas de masquer la moitié supérieure de chaque image d'entrée\n",
    "- sortie attendue :\n",
    "\n",
    "![Expected Output](ipm_assets/images/output_expected.png \"Cropped image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure you have loaded extrinsics_dict, intrinsics_dict\n",
    "E = extrinsics_dict['vr_1']\n",
    "K = intrinsics_dict['vr_1']\n",
    "config = {}\n",
    "config[\"px_per_m\"] = 10 # number of pixels per meter\n",
    "config[\"output_width\"] = 798\n",
    "config[\"output_height\"] = 400\n",
    "# shift to center of output image\n",
    "config[\"shift_x\"] = config[\"output_width\"] / 2.0 \n",
    "config[\"shift_y\"] = config[\"output_height\"] /2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_ipm(image, E, K, config):\n",
    "    # parameters for ipm\n",
    "    # output resolution\n",
    "    px_per_m = config[\"px_per_m\"] \n",
    "    # output size\n",
    "    width = config[\"output_width\"]\n",
    "    height = config[\"output_height\"]\n",
    "    # shift to center of the left edge of output image\n",
    "    shift_x = config[\"shift_x\"]\n",
    "    shift_y = config[\"shift_y\"]\n",
    "    \n",
    "    \n",
    "    input_img_height, input_img_width, _ = image.shape\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    mask = None\n",
    "    cv2.rectangle(None, None, None,  255, -1)\n",
    "    image = None\n",
    "    \n",
    "    # define matrix that maps from the road frame to the vehicle frame\n",
    "    \n",
    "    M_2Dto3D = None\n",
    "    M_direction = None\n",
    "    M_shift = None\n",
    "    M_scale = None\n",
    "    \n",
    "    M = None\n",
    "    \n",
    "    # define projection matrix\n",
    "    P = None\n",
    "    \n",
    "    M_ipm = None\n",
    "    \n",
    "    \n",
    "    img_out = None\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return img_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read image\n",
    "img = cv2.imread(\"ipm_assets/images/vr_1.png\")\n",
    "img = img[:, :, :]\n",
    "\n",
    "print(img.shape)\n",
    "# show image \n",
    "show_image(img, \"Original Image\")\n",
    "\n",
    "# apply ipm\n",
    "img_out2 = apply_ipm(img, E, K, config)\n",
    "# show output image \n",
    "show_image(img_out2, \"Output\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'utilisation d'une seule caméra ne donne pas une vue BEV à 360°. Nous pouvons appliquer l'IPM à des images provenant de caméras orientées dans différentes directions et assembler le résultat en BEV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tâche : Assembler plusieurs images en BEV\n",
    "Dans cette tâche, vous allez utiliser des images provenant de 8 caméras différentes.\n",
    "\n",
    "Vous allez charger plusieurs images depuis un dossier et utiliser les paramètres extrinsèques et intrinsèques de la caméra chargés depuis le fichier JSON pour appliquer la transformation de perspective inverse aux images.\n",
    "\n",
    "Vous allez implémenter une stratégie d'assemblage simple : simplement ajouter les images des différentes caméras en BEV.\n",
    "Remplacez les espaces réservés `None` par votre code.\n",
    "\n",
    "#### __Conseils :__\n",
    "- Sauvegardez les images dans un dictionnaire (par exemple `images_dict = {'vr_1': ...}`) avec la même clé utilisée pour les paramètres de la caméra\n",
    "- Créez un tableau numpy de zéros qui contiendra l'image BEV totale avec la forme : `config['output_height'], config['output_width'],3)`\n",
    "- Itérez sur toutes les images : (par exemple `for image_key, image_val in images_dict.items()` ...)\n",
    "- Ajoutez les images à l'image totale **uniquement aux positions qui sont encore** `= [0, 0, 0 ]` (pixels noirs)\n",
    "- Les pixels noirs dans une image peuvent être isolés en utilisant \"`image[image==(0,0,0)]`\"\n",
    "- sortie attendue :\n",
    "\n",
    "![Expected Output](ipm_assets/images/rgb_bev_360.png \"RGB BEV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images from folder\n",
    "images_directory = \"ipm_assets/cameras/images\"\n",
    "images = {}\n",
    "for filename in os.listdir(images_directory):\n",
    "    if filename.endswith(\".jpg\"):\n",
    "        image_pth = os.path.join(images_directory, filename)\n",
    "        #print(image_pth)\n",
    "        images[filename.split('.')[0]] = cv2.imread(image_pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply bev and add images \n",
    "def apply_ipm_and_stitch_images(images, config, extrinsics_dict, intrinsics_dict):\n",
    "    ### START CODE HERE ###\n",
    "    bev_total_img = None\n",
    "    for n, image in images.items():\n",
    "        output_image = None\n",
    "        #show_image(output_image, n)\n",
    "        bev_total_img[bev_total_img==(0,0,0)] = None\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    show_image(bev_total_img)                     \n",
    "        \n",
    "apply_ipm_and_stitch_images(images, config, extrinsics_dict, intrinsics_dict)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Tâche : Appliquer l'IPM à des images segmentées sémantiquement\n",
    "De la même manière que nous avons appliqué l'IPM à des images RGB, nous pouvons **l'appliquer à des images segmentées sémantiquement**. Les **images BEV résultantes seront également segmentées sémantiquement**.\n",
    "\n",
    "La tâche ici est d'effectuer une cartographie sémantique par grille basée sur caméra en utilisant l'IPM sur des images segmentées sémantiquement.\n",
    "Ici, vous utiliserez la même approche que nous avons utilisée pour les images de caméra normales.\n",
    "\n",
    "Exécutez les deux cellules suivantes, puis remplacez les espaces réservés `None` par votre code.\n",
    "\n",
    "#### __Conseils :__\n",
    "- sortie attendue :\n",
    "\n",
    "![Expected Output](ipm_assets/images/sem_bev_360.png \"RGB BEV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get extrinsic matrices dictionary\n",
    "extrinsics_dict = get_extrinsics(f_path=\"ipm_assets/carla_data/extrinsics_rpy.json\")\n",
    "\n",
    "intrinsics_dict = get_intrinsics(f_path=\"ipm_assets/carla_data/intrinsics.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images from folder\n",
    "images_directory = \"ipm_assets/carla_data/frames\"\n",
    "images = {}\n",
    "for filename in os.listdir(images_directory):\n",
    "    if filename.endswith(\".jpg\"):\n",
    "        image_pth = os.path.join(images_directory, filename)\n",
    "        #print(image_pth)\n",
    "        images[filename.split('.')[0]] = cv2.imread(image_pth)\n",
    "# list all images names in the dictionary \"images\"       \n",
    "print(images.keys())\n",
    "\n",
    "# show front right image \n",
    "show_image(images['front_right_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###        \n",
    "\n",
    "apply_ipm_and_stitch_images(None, None, None, None)\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Récapitulatif\n",
    "\n",
    "- Vous avez appris à appliquer la transformation de perspective inverse en utilisant OpenCV.\n",
    "- Vous avez appris à appliquer la transformation de perspective inverse en utilisant le modèle de caméra et les transformations de coordonnées.\n",
    "- Vous avez appris à assembler plusieurs images en vue aérienne.\n",
    "- Vous avez appris à calculer une cartographie sémantique par grille basée sur la géométrie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
